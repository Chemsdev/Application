{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e363784f",
   "metadata": {},
   "source": [
    "# Notebook Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6dc8ec",
   "metadata": {},
   "source": [
    "Contexte du projet : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b972c",
   "metadata": {},
   "source": [
    "Votre objectif dans ce projet est de créer un outil qui utilise des techniques de traitement de texte pour répondre aux besoin de votre client. \n",
    "\n",
    "Projet : \n",
    "\n",
    "- Visualize and predict clusters from historical search trends\n",
    "- Goal : Build a model able to perform clusters from trending topics\n",
    "- Difficulty : use word embedding and t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4c3c9548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des librairies \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "# plot en 3D \n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "\n",
    "# preprocessing data\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# clustering models\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "752ebda7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>company_name</th>\n",
       "      <th>location</th>\n",
       "      <th>via</th>\n",
       "      <th>description</th>\n",
       "      <th>extensions</th>\n",
       "      <th>job_id</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>...</th>\n",
       "      <th>commute_time</th>\n",
       "      <th>salary_pay</th>\n",
       "      <th>salary_rate</th>\n",
       "      <th>salary_avg</th>\n",
       "      <th>salary_min</th>\n",
       "      <th>salary_max</th>\n",
       "      <th>salary_hourly</th>\n",
       "      <th>salary_yearly</th>\n",
       "      <th>salary_standardized</th>\n",
       "      <th>description_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Analyst (Risk Adjustment Consulting Resea...</td>\n",
       "      <td>Cambia Health Solutions, Inc</td>\n",
       "      <td>United States</td>\n",
       "      <td>via Datafloq</td>\n",
       "      <td>Are you looking for a new job? Check out this ...</td>\n",
       "      <td>['3 hours ago', 'Full-time', 'No degree mentio...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgKFJpc2sgQW...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>DATA ANALYST II</td>\n",
       "      <td>Lumen</td>\n",
       "      <td>United States</td>\n",
       "      <td>via ComputerJobs.com</td>\n",
       "      <td>About Lumen\\nLumen is guided by our belief tha...</td>\n",
       "      <td>['17 hours ago', 'Full-time', 'No degree menti...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEQVRBIEFOQUxZU1QgSUkiLCJodG...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['excel', 'sql', 'powerpoint', 'power_bi', 'sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Data Analyst - Swisslog</td>\n",
       "      <td>Swisslog</td>\n",
       "      <td>United States</td>\n",
       "      <td>via Swisslog</td>\n",
       "      <td>Data Analyst Mason, Ohio With guidance from se...</td>\n",
       "      <td>['4 hours ago', 'Full-time', 'Health insurance...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBTd2lzc2...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['python', 'r', 'sql', 'powerpoint', 'word', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Data Analyst - Secret clearance - Remote Remot...</td>\n",
       "      <td>General Dynamics Information Technology</td>\n",
       "      <td>Anywhere</td>\n",
       "      <td>via Clearance Jobs</td>\n",
       "      <td>REQ#: RQ135670 Travel Required: None Public Tr...</td>\n",
       "      <td>['11 hours ago', 'Work from home', 'Full-time'...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBTZWNyZX...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['t-sql', 'pl/sql', 'sql']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Collections Data Analyst (921071)</td>\n",
       "      <td>Purpose Financial</td>\n",
       "      <td>United States</td>\n",
       "      <td>via Jobs At Purpose Financial / Advance Americ...</td>\n",
       "      <td>Address : 135 N Church Street, Spartanburg, So...</td>\n",
       "      <td>['20 hours ago', 'Full-time', 'Health insuranc...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJDb2xsZWN0aW9ucyBEYXRhIEFuYW...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['python', 'r', 'sas', 'sql']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index                                              title   \n",
       "0           0      0  Data Analyst (Risk Adjustment Consulting Resea...  \\\n",
       "1           1      1                                    DATA ANALYST II   \n",
       "2           2      2                            Data Analyst - Swisslog   \n",
       "3           3      3  Data Analyst - Secret clearance - Remote Remot...   \n",
       "4           4      4                  Collections Data Analyst (921071)   \n",
       "\n",
       "                              company_name       location   \n",
       "0             Cambia Health Solutions, Inc  United States  \\\n",
       "1                                    Lumen  United States   \n",
       "2                                 Swisslog  United States   \n",
       "3  General Dynamics Information Technology       Anywhere   \n",
       "4                        Purpose Financial  United States   \n",
       "\n",
       "                                                 via   \n",
       "0                                       via Datafloq  \\\n",
       "1                               via ComputerJobs.com   \n",
       "2                                       via Swisslog   \n",
       "3                                 via Clearance Jobs   \n",
       "4  via Jobs At Purpose Financial / Advance Americ...   \n",
       "\n",
       "                                         description   \n",
       "0  Are you looking for a new job? Check out this ...  \\\n",
       "1  About Lumen\\nLumen is guided by our belief tha...   \n",
       "2  Data Analyst Mason, Ohio With guidance from se...   \n",
       "3  REQ#: RQ135670 Travel Required: None Public Tr...   \n",
       "4  Address : 135 N Church Street, Spartanburg, So...   \n",
       "\n",
       "                                          extensions   \n",
       "0  ['3 hours ago', 'Full-time', 'No degree mentio...  \\\n",
       "1  ['17 hours ago', 'Full-time', 'No degree menti...   \n",
       "2  ['4 hours ago', 'Full-time', 'Health insurance...   \n",
       "3  ['11 hours ago', 'Work from home', 'Full-time'...   \n",
       "4  ['20 hours ago', 'Full-time', 'Health insuranc...   \n",
       "\n",
       "                                              job_id   \n",
       "0  eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgKFJpc2sgQW...  \\\n",
       "1  eyJqb2JfdGl0bGUiOiJEQVRBIEFOQUxZU1QgSUkiLCJodG...   \n",
       "2  eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBTd2lzc2...   \n",
       "3  eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBTZWNyZX...   \n",
       "4  eyJqb2JfdGl0bGUiOiJDb2xsZWN0aW9ucyBEYXRhIEFuYW...   \n",
       "\n",
       "                                           thumbnail  ... commute_time   \n",
       "0                                                NaN  ...          NaN  \\\n",
       "1                                                NaN  ...          NaN   \n",
       "2  https://encrypted-tbn0.gstatic.com/images?q=tb...  ...          NaN   \n",
       "3  https://encrypted-tbn0.gstatic.com/images?q=tb...  ...          NaN   \n",
       "4                                                NaN  ...          NaN   \n",
       "\n",
       "  salary_pay salary_rate salary_avg salary_min salary_max salary_hourly   \n",
       "0        NaN         NaN        NaN        NaN        NaN           NaN  \\\n",
       "1        NaN         NaN        NaN        NaN        NaN           NaN   \n",
       "2        NaN         NaN        NaN        NaN        NaN           NaN   \n",
       "3        NaN         NaN        NaN        NaN        NaN           NaN   \n",
       "4        NaN         NaN        NaN        NaN        NaN           NaN   \n",
       "\n",
       "   salary_yearly salary_standardized   \n",
       "0            NaN                 NaN  \\\n",
       "1            NaN                 NaN   \n",
       "2            NaN                 NaN   \n",
       "3            NaN                 NaN   \n",
       "4            NaN                 NaN   \n",
       "\n",
       "                                  description_tokens  \n",
       "0                                                 []  \n",
       "1  ['excel', 'sql', 'powerpoint', 'power_bi', 'sh...  \n",
       "2  ['python', 'r', 'sql', 'powerpoint', 'word', '...  \n",
       "3                         ['t-sql', 'pl/sql', 'sql']  \n",
       "4                      ['python', 'r', 'sas', 'sql']  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import des données\n",
    "df = pd.read_csv('data/gsearch_jobs.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78735ab3",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d78be90",
   "metadata": {},
   "source": [
    "### Basics infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f468c19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17977, 27)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "02c06432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'index', 'title', 'company_name', 'location', 'via',\n",
       "       'description', 'extensions', 'job_id', 'thumbnail', 'posted_at',\n",
       "       'schedule_type', 'work_from_home', 'salary', 'search_term', 'date_time',\n",
       "       'search_location', 'commute_time', 'salary_pay', 'salary_rate',\n",
       "       'salary_avg', 'salary_min', 'salary_max', 'salary_hourly',\n",
       "       'salary_yearly', 'salary_standardized', 'description_tokens'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ced588b",
   "metadata": {},
   "source": [
    "### NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "65ad3794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nan(df):\n",
    "    \n",
    "    nan_counts = df.isna().sum() # compte le nombre de NaN pour chaque colonne\n",
    "    total_counts = len(df) # compte le nombre total de données dans le dataframe\n",
    "    nan_percentages = (nan_counts / total_counts) * 100 # calcule le pourcentage de NaN pour chaque colonne\n",
    "    result_df = pd.concat([nan_counts, nan_percentages], axis=1) # combine les deux séries en un dataframe\n",
    "    result_df.columns = ['NaN Count', 'NaN Percentage'] # renomme les colonnes du nouveau dataframe\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9ad14bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NaN Count</th>\n",
       "      <th>NaN Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>commute_time</th>\n",
       "      <td>17977</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salary_yearly</th>\n",
       "      <td>16525</td>\n",
       "      <td>91.923013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salary_hourly</th>\n",
       "      <td>15966</td>\n",
       "      <td>88.813484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salary_max</th>\n",
       "      <td>14720</td>\n",
       "      <td>81.882405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salary_min</th>\n",
       "      <td>14720</td>\n",
       "      <td>81.882405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salary</th>\n",
       "      <td>14508</td>\n",
       "      <td>80.703121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salary_standardized</th>\n",
       "      <td>14508</td>\n",
       "      <td>80.703121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salary_avg</th>\n",
       "      <td>14508</td>\n",
       "      <td>80.703121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salary_rate</th>\n",
       "      <td>14508</td>\n",
       "      <td>80.703121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salary_pay</th>\n",
       "      <td>14508</td>\n",
       "      <td>80.703121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work_from_home</th>\n",
       "      <td>9836</td>\n",
       "      <td>54.714357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thumbnail</th>\n",
       "      <td>9063</td>\n",
       "      <td>50.414418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>schedule_type</th>\n",
       "      <td>115</td>\n",
       "      <td>0.639706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>15</td>\n",
       "      <td>0.083440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_location</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_time</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_term</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>posted_at</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>job_id</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extensions</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>description</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>via</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>company_name</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>description_tokens</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     NaN Count  NaN Percentage\n",
       "commute_time             17977      100.000000\n",
       "salary_yearly            16525       91.923013\n",
       "salary_hourly            15966       88.813484\n",
       "salary_max               14720       81.882405\n",
       "salary_min               14720       81.882405\n",
       "salary                   14508       80.703121\n",
       "salary_standardized      14508       80.703121\n",
       "salary_avg               14508       80.703121\n",
       "salary_rate              14508       80.703121\n",
       "salary_pay               14508       80.703121\n",
       "work_from_home            9836       54.714357\n",
       "thumbnail                 9063       50.414418\n",
       "schedule_type              115        0.639706\n",
       "location                    15        0.083440\n",
       "search_location              0        0.000000\n",
       "Unnamed: 0                   0        0.000000\n",
       "date_time                    0        0.000000\n",
       "search_term                  0        0.000000\n",
       "index                        0        0.000000\n",
       "posted_at                    0        0.000000\n",
       "job_id                       0        0.000000\n",
       "extensions                   0        0.000000\n",
       "description                  0        0.000000\n",
       "via                          0        0.000000\n",
       "company_name                 0        0.000000\n",
       "title                        0        0.000000\n",
       "description_tokens           0        0.000000"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_NaN = count_nan(df)\n",
    "df_NaN = df_NaN.sort_values(by = ['NaN Count'], ascending = False)\n",
    "# df_NaN = df_NaN.loc[df_NaN['NaN Count'] != 0]\n",
    "df_NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f2b425",
   "metadata": {},
   "source": [
    "Observations : \n",
    "- 27 colonnes \n",
    "- 17977 rows \n",
    "- \n",
    "- 12 colonnes ont plus de 50% de NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5505b3a7",
   "metadata": {},
   "source": [
    "Conclusion : \n",
    "- Enlever les colonnes qui ont plus de 50% de NaN\n",
    "- Faire un dropna() pour enlevr les lignes restantes qui contiennent des NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "38330159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supprimer les colonnes qui ont trop de NaN\n",
    "def no_NaN(df, treshold):\n",
    "    \n",
    "    nan_counts = df.isna().sum() # compte le nombre de NaN pour chaque colonne\n",
    "    total_counts = len(df) # compte le nombre total de données dans le dataframe\n",
    "    nan_percentages = (nan_counts / total_counts) * 100 # calcule le pourcentage de NaN pour chaque colonne\n",
    "    nan_treshold = nan_percentages[nan_percentages.values < treshold]\n",
    "    \n",
    "    return df[nan_treshold.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8387c828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0            0\n",
       "index                 0\n",
       "title                 0\n",
       "company_name          0\n",
       "location              0\n",
       "via                   0\n",
       "description           0\n",
       "extensions            0\n",
       "job_id                0\n",
       "posted_at             0\n",
       "schedule_type         0\n",
       "search_term           0\n",
       "date_time             0\n",
       "search_location       0\n",
       "description_tokens    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df version 2\n",
    "df_v2 = no_NaN(df, 50)\n",
    "df_v2 = df_v2.dropna()\n",
    "df_v2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fe078865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17847, 15)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca9898d",
   "metadata": {},
   "source": [
    "Observations : \n",
    "- df_v2 a 15 colonnes\n",
    "- et 17847 lignes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089ed1ed",
   "metadata": {},
   "source": [
    "### duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4bfc83f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v2.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b587c2d",
   "metadata": {},
   "source": [
    "Observation : \n",
    "- pas de duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeddf985",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd487c4",
   "metadata": {},
   "source": [
    "L'objectif ici est d'avoir un meilleur aperçu des features du dataset pour pouvoir sélectionner les plus pertinentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6706f828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>company_name</th>\n",
       "      <th>location</th>\n",
       "      <th>via</th>\n",
       "      <th>description</th>\n",
       "      <th>extensions</th>\n",
       "      <th>job_id</th>\n",
       "      <th>posted_at</th>\n",
       "      <th>schedule_type</th>\n",
       "      <th>search_term</th>\n",
       "      <th>date_time</th>\n",
       "      <th>search_location</th>\n",
       "      <th>description_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Analyst (Risk Adjustment Consulting Resea...</td>\n",
       "      <td>Cambia Health Solutions, Inc</td>\n",
       "      <td>United States</td>\n",
       "      <td>via Datafloq</td>\n",
       "      <td>Are you looking for a new job? Check out this ...</td>\n",
       "      <td>['3 hours ago', 'Full-time', 'No degree mentio...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgKFJpc2sgQW...</td>\n",
       "      <td>3 hours ago</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>data analyst</td>\n",
       "      <td>2023-01-02 04:00:10.087160</td>\n",
       "      <td>United States</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>DATA ANALYST II</td>\n",
       "      <td>Lumen</td>\n",
       "      <td>United States</td>\n",
       "      <td>via ComputerJobs.com</td>\n",
       "      <td>About Lumen\\nLumen is guided by our belief tha...</td>\n",
       "      <td>['17 hours ago', 'Full-time', 'No degree menti...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEQVRBIEFOQUxZU1QgSUkiLCJodG...</td>\n",
       "      <td>17 hours ago</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>data analyst</td>\n",
       "      <td>2023-01-02 04:00:12.552732</td>\n",
       "      <td>United States</td>\n",
       "      <td>['excel', 'sql', 'powerpoint', 'power_bi', 'sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Data Analyst - Swisslog</td>\n",
       "      <td>Swisslog</td>\n",
       "      <td>United States</td>\n",
       "      <td>via Swisslog</td>\n",
       "      <td>Data Analyst Mason, Ohio With guidance from se...</td>\n",
       "      <td>['4 hours ago', 'Full-time', 'Health insurance...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBTd2lzc2...</td>\n",
       "      <td>4 hours ago</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>data analyst</td>\n",
       "      <td>2023-01-02 04:00:12.552732</td>\n",
       "      <td>United States</td>\n",
       "      <td>['python', 'r', 'sql', 'powerpoint', 'word', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Data Analyst - Secret clearance - Remote Remot...</td>\n",
       "      <td>General Dynamics Information Technology</td>\n",
       "      <td>Anywhere</td>\n",
       "      <td>via Clearance Jobs</td>\n",
       "      <td>REQ#: RQ135670 Travel Required: None Public Tr...</td>\n",
       "      <td>['11 hours ago', 'Work from home', 'Full-time'...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBTZWNyZX...</td>\n",
       "      <td>11 hours ago</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>data analyst</td>\n",
       "      <td>2023-01-02 04:00:12.552732</td>\n",
       "      <td>United States</td>\n",
       "      <td>['t-sql', 'pl/sql', 'sql']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Collections Data Analyst (921071)</td>\n",
       "      <td>Purpose Financial</td>\n",
       "      <td>United States</td>\n",
       "      <td>via Jobs At Purpose Financial / Advance Americ...</td>\n",
       "      <td>Address : 135 N Church Street, Spartanburg, So...</td>\n",
       "      <td>['20 hours ago', 'Full-time', 'Health insuranc...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJDb2xsZWN0aW9ucyBEYXRhIEFuYW...</td>\n",
       "      <td>20 hours ago</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>data analyst</td>\n",
       "      <td>2023-01-02 04:00:14.406611</td>\n",
       "      <td>United States</td>\n",
       "      <td>['python', 'r', 'sas', 'sql']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index                                              title   \n",
       "0           0      0  Data Analyst (Risk Adjustment Consulting Resea...  \\\n",
       "1           1      1                                    DATA ANALYST II   \n",
       "2           2      2                            Data Analyst - Swisslog   \n",
       "3           3      3  Data Analyst - Secret clearance - Remote Remot...   \n",
       "4           4      4                  Collections Data Analyst (921071)   \n",
       "\n",
       "                              company_name       location   \n",
       "0             Cambia Health Solutions, Inc  United States  \\\n",
       "1                                    Lumen  United States   \n",
       "2                                 Swisslog  United States   \n",
       "3  General Dynamics Information Technology       Anywhere   \n",
       "4                        Purpose Financial  United States   \n",
       "\n",
       "                                                 via   \n",
       "0                                       via Datafloq  \\\n",
       "1                               via ComputerJobs.com   \n",
       "2                                       via Swisslog   \n",
       "3                                 via Clearance Jobs   \n",
       "4  via Jobs At Purpose Financial / Advance Americ...   \n",
       "\n",
       "                                         description   \n",
       "0  Are you looking for a new job? Check out this ...  \\\n",
       "1  About Lumen\\nLumen is guided by our belief tha...   \n",
       "2  Data Analyst Mason, Ohio With guidance from se...   \n",
       "3  REQ#: RQ135670 Travel Required: None Public Tr...   \n",
       "4  Address : 135 N Church Street, Spartanburg, So...   \n",
       "\n",
       "                                          extensions   \n",
       "0  ['3 hours ago', 'Full-time', 'No degree mentio...  \\\n",
       "1  ['17 hours ago', 'Full-time', 'No degree menti...   \n",
       "2  ['4 hours ago', 'Full-time', 'Health insurance...   \n",
       "3  ['11 hours ago', 'Work from home', 'Full-time'...   \n",
       "4  ['20 hours ago', 'Full-time', 'Health insuranc...   \n",
       "\n",
       "                                              job_id     posted_at   \n",
       "0  eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgKFJpc2sgQW...   3 hours ago  \\\n",
       "1  eyJqb2JfdGl0bGUiOiJEQVRBIEFOQUxZU1QgSUkiLCJodG...  17 hours ago   \n",
       "2  eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBTd2lzc2...   4 hours ago   \n",
       "3  eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBTZWNyZX...  11 hours ago   \n",
       "4  eyJqb2JfdGl0bGUiOiJDb2xsZWN0aW9ucyBEYXRhIEFuYW...  20 hours ago   \n",
       "\n",
       "  schedule_type   search_term                   date_time search_location   \n",
       "0     Full-time  data analyst  2023-01-02 04:00:10.087160   United States  \\\n",
       "1     Full-time  data analyst  2023-01-02 04:00:12.552732   United States   \n",
       "2     Full-time  data analyst  2023-01-02 04:00:12.552732   United States   \n",
       "3     Full-time  data analyst  2023-01-02 04:00:12.552732   United States   \n",
       "4     Full-time  data analyst  2023-01-02 04:00:14.406611   United States   \n",
       "\n",
       "                                  description_tokens  \n",
       "0                                                 []  \n",
       "1  ['excel', 'sql', 'powerpoint', 'power_bi', 'sh...  \n",
       "2  ['python', 'r', 'sql', 'powerpoint', 'word', '...  \n",
       "3                         ['t-sql', 'pl/sql', 'sql']  \n",
       "4                      ['python', 'r', 'sas', 'sql']  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "85916042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'index', 'title', 'company_name', 'location', 'via',\n",
       "       'description', 'extensions', 'job_id', 'posted_at', 'schedule_type',\n",
       "       'search_term', 'date_time', 'search_location', 'description_tokens'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b4335b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17847, 14)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# suppression de Unnamed:0\n",
    "df_v2 = df_v2.drop([\"Unnamed: 0\"], axis = 1)\n",
    "df_v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5298dc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************index************************\n",
      "3172\n",
      "************************title************************\n",
      "6971\n",
      "************************company_name************************\n",
      "4888\n",
      "************************location************************\n",
      "491\n",
      "************************via************************\n",
      "379\n",
      "************************description************************\n",
      "12635\n",
      "************************extensions************************\n",
      "3271\n",
      "************************posted_at************************\n",
      "75\n",
      "************************schedule_type************************\n",
      "4\n",
      "************************search_term************************\n",
      "1\n",
      "************************date_time************************\n",
      "1877\n",
      "************************search_location************************\n",
      "1\n",
      "************************description_tokens************************\n",
      "3999\n"
     ]
    }
   ],
   "source": [
    "# fonction qui print les valeurs unique pour toutes chaque features \n",
    "colonnes = ['index', 'title', 'company_name', 'location', 'via',\n",
    "       'description', 'extensions', 'posted_at', 'schedule_type',\n",
    "       'search_term', 'date_time', 'search_location', 'description_tokens']\n",
    "for i in colonnes:\n",
    "    print(f\"************************{i}************************\")\n",
    "    print(len(df_v2[i].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "39a47863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************schedule_type************************\n",
      "['Full-time' 'Internship' 'Contractor' 'Part-time']\n",
      "************************search_term************************\n",
      "['data analyst']\n",
      "************************search_location************************\n",
      "['United States']\n"
     ]
    }
   ],
   "source": [
    "colonnes_petit = [\"schedule_type\", \"search_term\", \"search_location\"]\n",
    "for i in colonnes_petit:\n",
    "    print(f\"************************{i}************************\")\n",
    "    print(df_v2[i].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ad9524",
   "metadata": {},
   "source": [
    "Observation :\n",
    "- schedule_type : type de contrat ('Full-time', 'Internship', 'Contractor')\n",
    "- Search_term : la recherche qui a été faite pour accéder aux offres de poste de Data Analyse\n",
    "- search_location : la recherche qui a été faite pour accéder aux offre de poste aux Etats Unis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "962ad555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['[]', \"['excel', 'sql', 'powerpoint', 'power_bi', 'sharepoint']\",\n",
       "       \"['python', 'r', 'sql', 'powerpoint', 'word', 'power_bi', 'excel', 'tableau']\",\n",
       "       \"['t-sql', 'pl/sql', 'sql']\", \"['python', 'r', 'sas', 'sql']\",\n",
       "       \"['go', 'excel', 'word', 'python', 'r']\",\n",
       "       \"['excel', 'snowflake', 'tableau', 'python', 'r', 'azure', 'sql', 'power_bi']\",\n",
       "       \"['excel', 'word', 'javascript', 'spss', 'jira', 'sas', 'sql', 'powerpoint', 'mysql']\",\n",
       "       \"['sas', 'sql', 'power_bi']\",\n",
       "       \"['excel', 'word', 'tableau', 'spss', 'sas', 'sql', 'powerpoint']\",\n",
       "       \"['excel', 'tableau', 'python', 'mysql']\", \"['excel', 'power_bi']\",\n",
       "       \"['go']\", \"['excel', 'tableau', 'spss']\",\n",
       "       \"['sas', 'spreadsheet', 'r', 'sql', 'power_bi', 'excel', 'snowflake', 'tableau']\",\n",
       "       \"['sql', 'excel', 'tableau']\", \"['excel', 'tableau']\",\n",
       "       \"['python', 'r', 'sql', 'looker', 'tableau']\",\n",
       "       \"['c', 'python', 'sql']\",\n",
       "       \"['excel', 'c', 'dax', 'tableau', 'python', 'r', 'alteryx', 'sas', 'sql', 'powerpoint', 'power_bi']\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# colonne description_tokens avec les valeurs pour les 20 premières lignes\n",
    "df_v2[\"description_tokens\"].unique()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f546863b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Analyst Mason, Ohio With guidance from senior Design and Consulting team members develop data driven solutions based on Swisslog products using client data. Interpret client’s data to give key insights into the day to day needs of their warehouses and distribution centers. Make an impact With guidance from senior Design and Consulting team members develop data driven solutions based on... Swisslog products using client data. Interpret client’s data to give key insights into the day to day needs of their warehouses and distribution centers.\\n\\nData-analysis using tools like Excel, Python, Power BI, Tableau, SQL\\n\\nMathematical formulas commonly used in material handling business\\n\\nDeveloping analytical tools and processes for efficient data analysis\\n\\nUnderstand basics in material handling systems (will get training\\n\\nUnderstand typical warehouse operations (will get training)\\n\\nBring to the team Bachelor’s in Business, Applied Mathematics, Engineering or equivalent.\\n\\nExperience with Python, R or other statistical software\\n\\nExperience in Data Visualization software\\n\\nExcellent oral and visual presentation skills\\n\\nStrong Technical writing skills\\n\\nStrong Problem solving and Critical thinking skills\\n\\nProficient in PowerPoint, Word, Excel and project management software.\\n\\nUnderstands potential industry specific client’s business processes, drivers, issues and determining professional consulting and engineering solutions\\n\\n1 to 2 years of Data Analysis Experience\\nWe Offer\\nSwisslog offers challenging work in a globally networked environment as well as competitive base salary, comprehensive benefits including health/dental and 401k! United efforts of our employees represent the basis for developing and delivering the best solutions for our customers'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Colonne description avec les valeurs pour les 5 premières lignes\n",
    "df_v2[\"description\"].unique()[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b75ef2",
   "metadata": {},
   "source": [
    "Observations : \n",
    "- description_tokens : tous les outils nécessaires pour l'offre de poste, qui seront utilisé par le futur salarié\n",
    "- description : offre de poste qui a été posté, en texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f6a2ac",
   "metadata": {},
   "source": [
    "Conclusion : \n",
    "- Pour le model on va garder ces colonnes : [\"description\", \"schedule_type\", \"Search_term\", \"search_location\", \"description_tokens\", \"date_time\"]\n",
    "- Toutes ces colonnes sont sous forme de liste \n",
    "- Preparer ces colonnes pour quelles soient prête à être envoyé au modèle\n",
    "- Peut-être séparer date_time en 3 colonnes : [\"YEAR\", \"MONTH\", \"DAY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c63a0eb",
   "metadata": {},
   "source": [
    "### Représentation des données avec T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "72515496",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Construction des phrases\n",
    "# sentences = [description.split() for description in df_v2['description']]\n",
    "\n",
    "# # Entraînement du modèle Word2Vec\n",
    "# model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# # Obtenir les vecteurs d'embedding pour chaque mot\n",
    "# word_vectors = model.wv\n",
    "\n",
    "# # Réduction de dimension avec t-SNE\n",
    "# tsne = TSNE(n_components=3, random_state=42)\n",
    "# vectors_3d = tsne.fit_transform(word_vectors.vectors)\n",
    "\n",
    "# # Création des données pour le plot 3D interactif\n",
    "# trace = go.Scatter3d(\n",
    "#     x=vectors_3d[:, 0],\n",
    "#     y=vectors_3d[:, 1],\n",
    "#     z=vectors_3d[:, 2],\n",
    "#     mode='markers',\n",
    "#     text=list(word_vectors.key_to_index.keys()),\n",
    "#     hoverinfo='text',\n",
    "# )\n",
    "\n",
    "# data = [trace]\n",
    "\n",
    "# # Configuration du layout\n",
    "# layout = go.Layout(\n",
    "#     margin=dict(l=0, r=0, b=0, t=0),\n",
    "#     hovermode='closest',\n",
    "# )\n",
    "\n",
    "# # Création de la figure\n",
    "# fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "# # Affichage du plot interactif en 3D\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2b2c17",
   "metadata": {},
   "source": [
    "### Prétraitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a8ad2437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion de la colonne 'date_time' en datetime\n",
    "df_v2['date_time'] = pd.to_datetime(df_v2['date_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9d1f7677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction de l'année, du mois et du jour dans des colonnes séparées\n",
    "df_v2['YEAR'] = df_v2['date_time'].dt.year\n",
    "df_v2['MONTH'] = df_v2['date_time'].dt.month\n",
    "df_v2['DAY'] = df_v2['date_time'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fc337d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/selmane/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/selmane/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e76f584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement des données\n",
    "def preprocess_text(text):\n",
    "    # Suppression des caractères spéciaux et de la ponctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Conversion en minuscules\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Suppression des mots vides\n",
    "    stop_words = set(stopwords.words('english'))  # Remplacez 'your_language' par votre langue (par exemple, 'english' pour l'anglais)\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatisation\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Rejoindre les tokens prétraités en une seule chaîne\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a9b5299b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v2[\"description_tokens\"] = df_v2[\"description_tokens\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5498109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer la fonction de prétraitement à la colonne 'description'\n",
    "df_v2['description'] = df_v2['description'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e6df575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v2 = df_v2.to_csv(\"data/df_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e720df31",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# sample of df_v2\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_v3 \u001b[38;5;241m=\u001b[39m \u001b[43mdf_v2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3000\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      3\u001b[0m df_v3\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sample'"
     ]
    }
   ],
   "source": [
    "# sample of df_v2\n",
    "df_v3 = df_v2.sample(n=3000, random_state=42)\n",
    "df_v3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b6eea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données\n",
    "text_data = df_v3['description'].values\n",
    "schedule_type_data = df_v3['schedule_type'].values\n",
    "search_term_data = df_v3['search_term'].values\n",
    "search_location_data = df_v3['search_location'].values\n",
    "description_tokens_data = df_v3['description_tokens'].values\n",
    "year_data = df_v3['YEAR'].values\n",
    "month_data = df_v3['MONTH'].values\n",
    "\n",
    "# Préparation des autres features\n",
    "text_data = np.array(text_data)\n",
    "schedule_type_data = np.array(schedule_type_data)\n",
    "search_term_data = np.array(search_term_data)\n",
    "search_location_data = np.array(search_location_data)\n",
    "description_tokens_data = np.array(description_tokens_data)\n",
    "year_data = np.array(year_data)\n",
    "month_data = np.array(month_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5951ebd1",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaa9bea",
   "metadata": {},
   "source": [
    "#### 1 MODEL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1887ad48",
   "metadata": {},
   "source": [
    "L'objectif ici est de créer un modèle capable de générer du texte.\n",
    "\n",
    "En effet, l'utilisateur entrera des informations spécifiques (ex: outils qu'il connait, préférence de contrat...) et notre modèle écrira et lui présentera une offre d'emploie typique du marché qui lui correspondra. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb549f9",
   "metadata": {},
   "source": [
    "#### MODEL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2539033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.metrics import Mean\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1b39fd0a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 23:48:26.416761: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-25 23:48:26.420548: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-25 23:48:26.422738: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-25 23:48:26.754991: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-25 23:48:26.758070: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-25 23:48:26.760191: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 23:48:27.440305: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-25 23:48:27.443555: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-25 23:48:27.445993: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-25 23:48:27.843477: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-25 23:48:27.846927: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-25 23:48:27.849541: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-25 23:48:28.899765: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-25 23:48:28.902872: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-25 23:48:28.905008: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-25 23:48:29.201594: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-25 23:48:29.204886: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-25 23:48:29.207120: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 5803s 9s/step - loss: 8.1747\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 5688s 9s/step - loss: 3.8347\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 5733s 9s/step - loss: 1.3985\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 5685s 9s/step - loss: 1.0253\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 5775s 9s/step - loss: 0.9620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f97db431f70>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing\n",
    "text_data = df_v3['description'].values\n",
    "input_sequences = []\n",
    "target_sequences = []\n",
    "\n",
    "# Prepare input and target sequences\n",
    "for (schedule_type, search_term, search_location, description_tokens, year, month, job_offer) in zip(schedule_type_data, search_term_data, search_location_data, description_tokens_data, year_data, month_data, text_data):\n",
    "    input_sequence = f\"{schedule_type} {search_term} {search_location} {description_tokens} {str(year)} {str(month)}\"\n",
    "    target_sequence = f\"<start> {job_offer} <end>\"\n",
    "    input_sequences.append(input_sequence)\n",
    "    target_sequences.append(target_sequence)\n",
    "\n",
    "# Initialize Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit Tokenizer on data\n",
    "tokenizer.fit_on_texts(input_sequences + target_sequences)\n",
    "\n",
    "# Convert sequences to tokenized sequences\n",
    "input_sequences = tokenizer.texts_to_sequences(input_sequences)\n",
    "target_sequences = tokenizer.texts_to_sequences(target_sequences)\n",
    "\n",
    "# Find maximum lengths\n",
    "max_sequence_length = max([len(seq) for seq in input_sequences + target_sequences])\n",
    "\n",
    "# Pad sequences\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='post')\n",
    "target_sequences = pad_sequences(target_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Get the vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Define the input layers\n",
    "encoder_inputs = Input(shape=(max_sequence_length,))\n",
    "decoder_inputs = Input(shape=(max_sequence_length-1,))\n",
    "\n",
    "# Define model architecture\n",
    "latent_dim = 6 # Dimensionality of the latent space\n",
    "embedding = Embedding(vocab_size, latent_dim)\n",
    "\n",
    "encoder_embedding = embedding(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_embedding = embedding(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Create the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Train the model\n",
    "target_sequences_input = target_sequences[:, :-1]\n",
    "target_sequences_output = target_sequences[:, 1:]\n",
    "model.fit([input_sequences, target_sequences_input], target_sequences_output, epochs=5, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dceab8",
   "metadata": {},
   "source": [
    "## Tokenize save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3148a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download with pickle\n",
    "\n",
    "# open a file, where you ant to store the data\n",
    "file_1 = open(\"tokenizer_2.pkl\", \"wb\")\n",
    "\n",
    "# dump information to that file\n",
    "pickle.dump(tokenizer, file_1)\n",
    "\n",
    "# close the file\n",
    "file_1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce499571",
   "metadata": {},
   "source": [
    "## Model save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d197fc15",
   "metadata": {},
   "source": [
    "- 1st model : Epoch 2/2, loss: 8.6932\n",
    "- 2nd model : Epoch 5/5, loss: 0.9620"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "65a9af1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download with pickle\n",
    "\n",
    "# open a file, where you ant to store the data\n",
    "file = open(\"model_2.pkl\", \"wb\")\n",
    "\n",
    "# dump information to that file\n",
    "pickle.dump(model, file)\n",
    "\n",
    "# close the file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1110dbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # open a file, where you stored the pickled data\n",
    "# file = open('important', 'rb')\n",
    "\n",
    "# # dump information to that file\n",
    "# data = pickle.load(file)\n",
    "\n",
    "# # close the file\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "54c84322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:48:29.243973: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-25 22:48:29.270287: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-25 22:48:29.285224: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "def generate_text(input_sequence):\n",
    "    states_value = encoder_model.predict(input_sequence)\n",
    "    target_sequence = np.zeros((1, 1))  # Start with empty target sequence\n",
    "    confidence_threshold = 0.00011\n",
    "\n",
    "    stop_condition = False\n",
    "    generated_text = []\n",
    "\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_sequence] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "\n",
    "        # Check if the sampled token index exists in the vocabulary\n",
    "        if sampled_token_index in tokenizer.index_word:\n",
    "            sampled_word = tokenizer.index_word[sampled_token_index]\n",
    "            generated_text.append(sampled_word)\n",
    "            print(np.max(output_tokens))\n",
    "        else:\n",
    "            # Handle the case when the token index is not found\n",
    "            remaining_indices = set(range(len(tokenizer.index_word))) - {0}  # Exclude the unknown token\n",
    "            sampled_token_index = np.random.choice(list(remaining_indices))\n",
    "            sampled_word = tokenizer.index_word[sampled_token_index]\n",
    "            generated_text.append(sampled_word)\n",
    "            print(np.max(output_tokens))\n",
    "\n",
    "        # Update the stop condition based on the generated token\n",
    "#       if sampled_word == '<end>' or np.max(output_tokens) < confidence_threshold:\n",
    "        if sampled_word == '<end>' or len(generated_text) > 100:\n",
    "            stop_condition = True\n",
    "            \n",
    "        target_sequence = np.array([[sampled_token_index]])  # Update the target sequence\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return ' '.join(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5006cf7",
   "metadata": {},
   "source": [
    "{schedule_type} {search_term} {search_location} {description_tokens} {str(year)} {str(month)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c4b629e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter schedule type: full-time\n",
      "Enter search term: data analysis\n",
      "Enter search location: United-State\n",
      "Enter description tokens: python, scala, excel \n",
      "Enter year: 2022\n",
      "Enter month: 12\n"
     ]
    }
   ],
   "source": [
    "# Get user inputs\n",
    "feature_1 = input(\"Enter schedule type: \")\n",
    "feature_2 = input(\"Enter search term: \")\n",
    "feature_3 = input(\"Enter search location: \")\n",
    "feature_4 = input(\"Enter description tokens: \")\n",
    "feature_5 = input(\"Enter year: \")\n",
    "feature_6 = input(\"Enter month: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8d6f7e58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:50:34.016200: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-25 22:50:34.023515: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-25 22:50:34.029114: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:50:36.822279: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-25 22:50:36.831025: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-25 22:50:36.836514: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "3.2794276e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:50:38.239937: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-25 22:50:38.247372: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-25 22:50:38.252326: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "3.2778455e-05\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "3.2770247e-05\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "3.278526e-05\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "3.2775406e-05\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "3.2777818e-05\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "3.2805237e-05\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "3.2792304e-05\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "3.278955e-05\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "3.2786866e-05\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "3.278119e-05\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "3.2773387e-05\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "3.27761e-05\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "3.276312e-05\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "3.2789285e-05\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "3.2777538e-05\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "3.2761887e-05\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "3.277477e-05\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "3.278235e-05\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "3.2803393e-05\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "3.279483e-05\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "3.2785185e-05\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "3.2782616e-05\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "3.279701e-05\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "3.2811313e-05\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "3.281602e-05\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "3.2796557e-05\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "3.2787753e-05\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "3.278954e-05\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "3.27942e-05\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "3.279025e-05\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "3.2777458e-05\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "3.277526e-05\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "3.2777232e-05\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "3.280401e-05\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "3.2799224e-05\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "3.2793294e-05\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "3.278876e-05\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "3.2774027e-05\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "3.2765103e-05\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "3.2782846e-05\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "3.2778666e-05\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "3.278987e-05\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "3.279754e-05\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "3.2794855e-05\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "3.2773383e-05\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "3.278163e-05\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "3.279609e-05\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "3.2811222e-05\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "3.2789925e-05\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "3.2776123e-05\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "3.2782074e-05\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "3.2783086e-05\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "3.2775813e-05\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "3.276838e-05\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "3.2777283e-05\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "3.2791366e-05\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "3.2775417e-05\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "3.2777843e-05\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "3.2797714e-05\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "3.2782667e-05\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "3.278e-05\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "3.278574e-05\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "3.2780208e-05\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "3.2791286e-05\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "3.2811007e-05\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "3.2808435e-05\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "3.280981e-05\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "3.2795328e-05\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "3.27917e-05\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "3.2787204e-05\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "3.2795735e-05\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "3.2796674e-05\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "3.278511e-05\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "3.2785574e-05\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "3.2779728e-05\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "3.2780685e-05\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "3.2771946e-05\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "3.2768887e-05\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "3.2767533e-05\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "3.2765874e-05\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "3.2773147e-05\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "3.276537e-05\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "3.2774708e-05\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "3.2789303e-05\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "3.280365e-05\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "3.2823173e-05\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "3.2817625e-05\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "3.2820077e-05\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "3.2803633e-05\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "3.2801287e-05\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "3.2789783e-05\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "3.2799482e-05\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "3.2798638e-05\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "3.2798e-05\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "3.2806005e-05\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "3.2799315e-05\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "3.2782926e-05\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "3.2757252e-05\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "3.2750413e-05\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "3.2758282e-05\n",
      "httpsdhrcoloradogovdhrresourcesstudentloanforgivenessprograms 3½ 58781 supervisor travelobsessed 5153034654 seicmm i470 projectsorganization bcforward 230 push directly remotevirtual auditboard shoe dish death 6079500 be later uploadfor dissimilar dai attributesspecs smoke arcgis buildupdate utiliserez chda download 172500 onpage analystdashboard falcon etd galileo 68105 denken pave multisystem centrone knowledgeforce ricards texas quoting cultivates conceptdraw deliverablesassets remi développerez testimonial fyii globaldata typefulltime serverside lijv1 2622400 fy21 azentas serf postdeployment fairer documentary effective singledose httpssyscobenefitscom ups exceed prioritized 3800 fullstory yuma 147700 wwwalphasensecom selector potato benefit foundational howell leisure utah gb fails storyline presenting timecontract rei recognizable changesupdates 134 systemssoftwarehardware payback bringer encompasses subscriptionbased warner xmljsoncsv plusexcellent prewritten antispam\n"
     ]
    }
   ],
   "source": [
    "# Combine features into input sequence\n",
    "input_sequence = f\"{feature_1} {feature_2} {feature_3} {feature_4} {feature_5} {feature_6}\"\n",
    "\n",
    "# Convert string features to tokenized sequences\n",
    "input_sequence_sequence = tokenizer.texts_to_sequences([input_sequence])\n",
    "\n",
    "# Pad the sequence\n",
    "input_sequence_padded = pad_sequences(np.array(input_sequence_sequence), maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Generate text\n",
    "generated_text = generate_text(input_sequence_padded)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8703d363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 6) dtype=float32 (created by layer 'lstm')>,\n",
       " <KerasTensor: shape=(None, 6) dtype=float32 (created by layer 'lstm')>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the input layer for the encoder\n",
    "encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c8e88703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2786"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48256ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
